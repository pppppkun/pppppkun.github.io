---
layout:     post
title:      CS229-支持向量机
subtitle:   机器学习
date:       2019-12-15
author:     Pkun
header-img: img/stfml.jpg
catalog: true
tags:
    - 机器学习
---

# CS229 课程讲义中文翻译

大部分内容来自CS229吴恩达老师的课程讲义，感谢翻译

| 原作者 | 翻译 | 校对 |
| --- | --- | --- |
| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |


|相关链接|
|---|
|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|
|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|
|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|
|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|

## 边界

在逻辑回归中，我们会对预测值大于等于0.5的样本当作正样本，小于0.5的当作负样本，直觉上，如果我们的预测值越大，这个样本实际上真的是一个正样本的可能性也就越大，我们对预测他为正样本也更有信心。

对于一个给定的训练集，如果我们能找到一个 $\theta$，满足当 $y^{(i)} = 1$ 的时候总有 $\theta^T x^{(i)} \gg 0$，而 $y^{(i)} = 0$ 的时候则 $\theta^T x^{(i)} \ll 0$，我们就说这个对训练数据的拟合很好，因为这就能对所有训练样本给出可靠（甚至正确）的分类。

这正是我们想要的效果

### 记号

在讨论 SVMs 的时候，出于简化的目的，我们先要引入一个新的记号，用来表示分类。假设我们要针对一个二值化分类的问题建立一个线性分类器，其中用来分类的标签(label)为 $y$，分类特征(feature)为 $x$。从此以后我们就用 $y \in \{{-1},1\}$ （而不是之前的 $\{0, 1\}$） 来表示这个分类标签了。另外，以后咱们也不再使用向量 $\theta$ 来表示咱们这个线性分类器的参数了，而是使用参数 $w$ 和 $b$，把分类器写成下面这样：

$$
h_{w,b}(x)=g(w^Tx+b)
$$

当 $z \geq 0$，则 $g(z) = 1$；而反之若 $z ＜ 0$，则$g(z) = -1$。这里的这个 “$w, b$” 记号就可以让我们能把截距项(intercept term)$b$ 与其他的参数区别开。（此外我们也不用再像早些时候那样要去设定 $x_0 = 1$ 这样的一个额外的输入特征向量了。）所以，这里的这个参数 $b$ 扮演的角色就相当于之前的参数 $\theta_0$ ，而参数 $w$ 则相当于 $[\theta_1 \dots \theta_n]^T$。

还要注意的是，从我们上面对函数 $g$ 的定义，可以发现我们的分类器给出的预测是 $1$ 或者 $-1$ （参考 感知器算法 perceptron algorithm），这样也就不需要先通过中间步骤(intermediate step)来估计 $y$ 为 $1$ （这里指逻辑回归中的步骤）的概率。

## 函数边界和几何边界

### 函数边界

给定一个训练集 $(x^{(i)}, y^{(i)})$，我们用下面的方法来定义对应该训练集的**函数边界** $(w, b)$：

$$
\hat\gamma^{(i)}=y^{(i)}(w^Tx+b)
$$

只要满足 $y^{(i)}(w^T x + b) \ge 0$，那我们针对这个样本的预测就是正确的。因此，一个大的函数边界就表示了一个可信且正确的预测。

对于一个线性分类器，选择上面给定的函数 $g$ ，函数边界的一个性质却使得这个分类器并不具有对置信度的良好量度。例如上面给定的这个函数 $g$，我们会发现，如果用 $2w$ 替换掉 $w$，然后用 $2b$ 替换 $b$，那么由于有 $g(w^Tx + b) = g(2w^Tx + 2b)$，这样改变也并不会影响 $h_{w,b}(x)$。也就是说，函数 $g$ 以及 $h_{w,b}(x)$ 只取决于 $w^T x + b$ 的正负符号(sign)，而不受其大小(magnitude)的影响。然而，把$(w, b)$ 翻倍成 $(2w,2b)$ 还会导致函数距离也被放大了 $2$ 倍。因此，这样看来就是只要随意去调整 $w$ 和 $b$ 的范围，我们就可以人为调整函数边界到足够大了，而不用去改变任何有实际意义的变量。直观地看，这就导致我们有必要引入某种归一化条件，例如使 $\parallel w\parallel_2 = 1$；也就是说，我们可以将 $(w, b)$ 替换成 $(w/\parallel w\parallel _2,b/\parallel w\parallel _2)$，然后考虑对应 $(w/\parallel w\parallel_2,b/\parallel w\parallel_2)$ 的函数边界。我们稍后再详细讨论这部分内容。

给定一个训练集 $S = \{(x^{(i)},y^{(i)}); i = 1, ..., m\}$，我们将对应 $S$ 的函数边界 $(w, b)$ 定义为每个训练样本的函数边界的最小值。记作 $\hat \gamma$，可以写成：

$$
\hat\gamma= \min_{i=1,...,m}\hat\gamma^{(i)}
$$

### 几何边界

接下来，咱们要讲的是**几何边界(geometric margins)。** 例如下图所示：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note3f2.png)

图中给出了对应 $(w, b)$ 的分类边界，其倾斜方向（即法线方向）为向量 $w$ 的方向。这里的向量 $w$ 是与**分类超平面**垂直的向量。假设有图中所示的一个点 $A$，此点表示的是针对某训练样本的输入特征为 $x^{(i)}$ ，对应的标签(label)为 $y^{(i)} = 1$。然后这个点到分类边界的距离 $\gamma^{(i)}$， 就通过 $AB$ 之间的线段能够获得。

怎么找到的 $\gamma^{(i)}$ 值呢？这样，$ w/\parallel w\parallel$ 是一个单位长度的向量，指向与 $w$ 相同的方向。因为这里 $A$ 点表示的是 $x^{(i)}$，所以就能找到一个点 $B$，其位置为 $x^{(i)} - \gamma^{(i)} \times w/\parallel w\parallel$。这个 $B$ 点正好位于分类边界线上面，而这条线上的所有 $x$ 都满足等式 $w^T x + b = 0$ ，所以有：

$$
w^T(x^{(i)}-\gamma^{(i)}\frac{w}{\parallel w\parallel })+b=0
$$

通过上面的方程解出来的 $\gamma^{(i)}$ 为：

$$
\gamma^{(i)}=\frac{w^Tx^{(i)}+b}{\parallel w\parallel }=(\frac{w}{\parallel w\parallel })^Tx^{(i)}+\frac{b}{\parallel w\parallel }
$$

这个解是针对图中 $A$ 处于训练样本中正向部分这种情况，这时候位于“正向(positive)”一侧就是很理想的情况。如果更泛化一下，就可以定义对应训练样本 $(x^{(i)}, y^{(i)})$ 的几何边界 $(w, b)$ 为：

$$
\gamma^{(i)}=y^{(i)}((\frac{w}{\parallel w\parallel })^Tx^{(i)}+\frac{b}{\parallel w\parallel })
$$

如果 $\parallel w\parallel  = 1$，那么函数边界就等于几何边界，我们可以用这种方法来将两个边界记号联系起来。此外，几何边界是不受参数缩放的影响的；也就是说，如果我们把 $w$ 改为 $2w$，$b$ 改为 $2b$，那么几何边界并不会改变。稍后这个性质就会派上用场了。特别要注意的是，由于这个与参数缩放的无关性，当试图对某个数据集的 $w$ 和 $b$ 进行拟合的时候，我们就可以倒入一个任意设置的缩放参数来约束 $w$，而不会改变什么重要项；例如，我们可以设置 $\parallel w\parallel  = 1$，或者 $|w_1| = 5$，或者 $|w_1 +b|+|w_2| = 2$，等等都可以，这些都只需要对 $w$ 和 $b$ 进行缩放就可以满足了。

最后，给定一个训练集 $S = \{(x^{(i)}, y^{(i)}); i = 1, ..., m\}$，我们也可以我们将对应 $S$ 的几何边界 $(w, b)$ 定义为每个训练样本的几何边界的最小值：

$$
\gamma=\min_{i=1,...,m}\gamma^{(i)}
$$

## 最优边界分类器（上）

给定一个训练集，根据咱们前文的讨论，似乎很自然地第一要务就是要尝试着找出一个分类边界，使（几何）边界能够最大，因为这会反映出对训练集进行的一系列置信度很高的分类预测，也是对训练数据的一个良好“拟合(fit)”。这样生成的一个分类器，能够把正向和负向的训练样本分隔开，中间有一个“空白区(gap)”，也就是几何边界。

到目前为止，我们都是假定给定的训练集是线性可分(linearly separable)的；也就是说，能够在正向和负向的样本之间用某种分类超平面来进行划分。那要怎样找到能够得到最大几何边界的那一组呢？我们可以提出下面的这样一个优化问题(optimization problem)：

$$
\begin{aligned}
max_{\gamma,w,b} \quad& \gamma \\
s.t. \quad &y^{(i)}(w^Tx^{(i)}+b) \geq \gamma,\quad i=1,...,m\\
&\parallel w\parallel =1 \\
\end{aligned}
$$

也就是说，我们要让 $\gamma$ 取最大值，使得每一个训练样本的函数边界都至少为 $\gamma$。另外 $\parallel w\parallel  = 1$ 这个约束条件还能保证函数边界与几何边界相等。因此，对上面这个优化问题进行求解，就能得出对应训练集的最大可能几何边界的 $(w, b)$。

### 第一次转换问题

如果解出来上面的优化问题，那就全都搞定了。但 “$\parallel w\parallel  = 1$” 这个约束条件很讨厌，是非凸的(non-convex)。所以我们要把这个问题进行改善，让它更好解。例如：

$$
\begin{aligned}
max_{\hat\gamma,w,b} \quad& \frac{\hat \gamma}{\parallel w\parallel } \\
s.t. \quad &y^{(i)}(w^Tx^{(i)}+b) \geq \gamma,\quad i=1,...,m\\
\end{aligned}
$$

这时候，我们要让 $\hat \gamma/\parallel w\parallel$ 的取值最大，使得函数边界都至少为 $\hat \gamma$。由于几何边界和函数边界可以通过 $\gamma = \hat \gamma/\parallel w\parallel$ 来联系起来，所以这样就能得到我们想要的结果了。而且，这样还能摆脱掉 $\parallel w\parallel  = 1$ 这个讨厌的约束条件。然而，悲剧的是我们现在就有了一个很讨厌的（还是非凸的）目标函数 $\hat \gamma/\parallel w\parallel$；而且，我们还是没有什么现成的软件(off-the-shelf software)能够解出来这样的一个优化问题。

### 第二次转换问题

那接着看吧。还记得咱们之前讲过的可以对 $w$ 和 $b$ 设置任意的一个缩放约束参数，而不会改变任何实质性内容。咱们现在就要用到这个重要性质了。下面咱们就来引入一个缩放约束参数，这样针对训练集的函数边界 $\hat \gamma$ 的这个参数就可以设置为 $1$：

$$
\hat \gamma =1
$$

对 $w$ 和 $b$ 使用某些常数来进行翻倍，结果就是函数边界也会以相同的常数进行加倍，这就确实是一个缩放约束了，而且只要对 $w$ 和 $b$ 进行缩放就可以满足。把这个性质用到咱们上面的优化问题中去，同时要注意到当 $\hat \gamma/\parallel w\parallel  = 1/\parallel w\parallel$ 取得最大值的时候，$\parallel w\parallel ^2$ 取得最小值，所以就得到了下面的这个优化问题：

$$
\begin{aligned}
min_{\gamma,w,b} \quad& \frac{1}{2}\parallel w\parallel ^2 \\
s.t. \quad &y^{(i)}(w^Tx^{(i)}+b) \geq 1,\quad i=1,...,m\\
\end{aligned}
$$

通过上面这样的转换，这个问题就变得容易解决了。上面的问题有一个凸二次对象，且仅受线性约束(only linear constraints)。对这个问题进行求解，我们就能得到**最优边界分类器(optimal margin classifier)。** 这个优化问题的求解可以使用商业二次规划(commercial quadratic programming ，缩写QP)代码$^1$。

>1 可能你更熟悉的是线性规划(linear programming)，这种方法适用的优化问题是有线性对象(linear objectives)和线性约束(linear constraints)。QP 软件的适用范围也很广泛，其中就包括这种凸二次对象(convex quadratic objectives)和线性约束的情况。

> 这样，差不多就可以说问题已经得到了解决，接下来咱们就要岔开话题，聊一聊拉格朗日对偶性(Lagrange duality)。这样就会引出我们这个优化问题的对偶形式(dual form)，这种形式会在我们后续要使用核(kernels)的过程中扮演重要角色，核(kernels)可以有效地对极高维度空间中的数据建立最优边界分类器。通过这种对偶形式，我们还能够推出一种非常有效的算法，来解决上面这个优化问题，而且通常这个算法那还能比通用的 QP 软件更好用。

## 拉格朗日对偶性(Lagrange duality)

咱们先把 SVMs 以及最大化边界分类器都放到一边，先来谈一下约束优化问题的求解。

例如下面这样的一个问题：

$$
\begin{aligned}
min_w \quad & f(w)& \\
s.t. \quad &h_i(w) =0,\quad i=1,...,l\\
\end{aligned}
$$

这个问题可以使用拉格朗日乘数法来解决。在这个方法中，我们定义了一个**拉格朗日函数(Lagrangian)** 为：

$$
L(w,\beta)=f(w)+\sum^l_{i=1}\beta_i h_i(w)
$$

上面这个等式中，这个 $\beta_i$ 就叫做**拉格朗日乘数。** 然后接下来让 对 $L$ 取偏导数，使其为零：

$$
\frac{\partial L }{\partial w_i} =0; \quad \frac{\partial L }{\partial \beta_i} =0;
$$

然后就可以解出对应的 $w$ 和 $\beta$ 了。在本节，我们对此进行一下泛化，扩展到约束优化的问题上，其中同时包含不等约束和等式约束。

>由于篇幅限制，我们在本课程$^2$不能讲清楚全部的拉格朗日对偶性，但还是会给出主要的思路和一些结论的，这些内容会用到我们稍后的最优边界分类器的优化问题。

>2 对拉格朗日对偶性该兴趣的读者如果想要了解更多，可以参考阅读 R. T. Rockefeller (1970) 所作的《凸分析》(Convex Analysis)，普林斯顿大学出版社(Princeton University Press)。

### 主最优化问题

下面这个，我们称之为**主** 最优化问题(primal optimization problem)：

$$
\begin{aligned}
min_w \quad & f(w)& \\
s.t. \quad & g_i(w) \le 0,\quad i=1,...,k\\
& h_i(w) =0,\quad i=1,...,l\\
\end{aligned}
$$

要解决上面这样的问题，首先要定义一下**广义拉格朗日函数(generalized Lagrang_ian)：**

$$
L(w,\alpha,\beta)=f(w)+\sum^k_{i=1}\alpha_ig_i(w)+\sum^l_{i=1}\beta_ih_i(w)
$$

上面的式子中， $\alpha_i$ 和 $\beta_i$ 都是**拉格朗日乘数(Lagrange multipliers)**。设有下面这样一个量(quantity)：

$$
\theta_{P}(w)=\max_{\alpha,\beta:\alpha_i \geq 0}L(w,\alpha,\beta)
$$

上式中的 $“P”$ 是对 “primal” 的简写。设已经给定了某些 $w$。如果 $w$ 不能满足某些主要约束，（例如对于某些 $i$ 存在 $g_i(w) > 0$ 或者 $h_i(w) \neq 0$），那么咱们就能够证明存在下面的等式关系：

$$
\begin{aligned}
\theta_P(w)&=\max_{\alpha,\beta:\alpha_i \geq 0} f(w)+\sum^k_{i=1}\alpha_ig_i(w)+\sum^l_{i=1}\beta_ih_i(w) &\text{(1)}\\
&= \infty &\text{(2)}\\
\end{aligned}
$$

因为我们可以取一些很小很小的$\alpha$或者很大很大的$\beta$来让$\theta_P(w)=\infty$

与之相反，如果 $w$ 的某些特定值确实能满足约束条件，那么则有 $\theta_P(w) = f(w)$。因此总结一下就是：

$$
\theta_P(w)= \begin{cases} f(w) & \text {if w satisfies primal constraints} \\
\infty & \text{otherwise} \end{cases}
$$

因此，如果 $w$ 的所有值都满足主要约束条件，那么$\theta_P$的值就等于此优化问题的目标量，而如果约束条件不能被满足，那 $\theta_P$的值就是正无穷了。所以，进一步就可以引出下面这个最小化问题：

$$
\min_w \theta_P(w)=\min_w \max_{\alpha,\beta:\alpha_i\geq0} L(w,\alpha,\beta)
$$

这个新提出的问题与之前主要约束问题有一样的解，所以还是同一个问题。为了后面的一些内容，我们要在这里定义一个目标量的最优值$p ^\ast = min_w \theta_P (w)$；我们把这个称为 主要优化问题的**值** (value of the primal problem)。

