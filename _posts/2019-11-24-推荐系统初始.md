---
layout:     post
title:      推荐系统初识
subtitle:   机器学习
date:       2019-11-24
author:     Pkun
header-img: img/stfml.jpg
catalog: true
tags:
    - 机器学习
    - 推荐系统
---


# 推荐系统

推荐系统即让系统根据已有的数据，比如用户的兴趣特点和购买行为，来判断用户对什么产品感兴趣，向用户推荐感兴趣的信息和商品，帮助用户决定该购买什么商品。


## 问题形式化

按照机器学习基本的思路，我们应该对问题建模，然后对比模型和真实情况的差异，通过让差异最小化来帮助我们找到最佳的参数。对于推荐系统也是一样的。

我们以豆瓣为例，假如我们有5部电影和4个用户

Movie|Alice|Bob|Carol|Dave
---|:--:|:--:|:--:|---:
Love at Last|5|5|0|0
Romanceforever|5|？|？|0
Cute puppies of love|？|4|0|？
Nonstop car chases|0|0|5|4
Swords vs. karate|0|0|5|?

前三部是爱情片，后两部是动作片

通过表格我们很容易可以看出Alice和Bob比较喜欢爱情片，而Carol和Dave似乎比较喜欢动作片，同时没有一个用户同时给所有电影打过分。

根据我们刚刚的陈述，我们希望根据已有的数据构建一个算法，然后通过算法预测出他们对未打分电影的评价，以此为他们推荐电影。


## 基于内容的推荐系统

我们刚刚根据不同电影的类别来判断了不同用户对不同类型电影的喜好，基于此，我们可以想到为每一部电影都建立一个特征向量，用这个特征向量来表示电影的特征，那我们如何表示用户对某部电影的喜好程度呢？

结合刚刚的栗子，我们应该给用户建立一个参数向量，来表示他们对不同类别电影的偏爱程度。


### 单用户的代价函数

设用户$j$的参数向量为$\theta ^ j$，第$i$部电影的特征值为$x^i$，就像我们在线性回归中所作的那样，我们很自然的考虑如下的代价函数

$$
\min_{\theta^j} \frac{1}{2} \sum_{i:r(i,j)=1} ((\theta^{j})^Tx^{i}-y^{(i,j)})^2
$$

其中$i:r(i,j)=1$表示我们只计算用户$j$所看过的那些电影，注意的是在一般的线性回归中我们会乘上$\frac{1}{2m}$，但是这里我们没有这么做，一是因为扩大缩小常数倍是不影响极值点的，二是我们只计算了部分电影，加上参数会带来一些因为数值较小而遇到的麻烦。

### 所有用户的代价函数

只需对所有的用户求和即可

$$
\min_{\theta^1,...,\theta^{n_u}} \frac{1}{2} \sum_{j=1}^{n_u}\sum_{i:r(i,j)=1} ((\theta^{j})^Tx^{i}-y^{(i,j)})^2
$$

用梯度下降法来更新参数只需要求一次导即可，比较简单，就不多写了

## 协同过滤

在之前基于内容的推荐系统中，我们根据电影的特征训练出了用户参数，同样的，我们可以根据用户参数训练出电影的特征

$$
\min_{x^1,...,x^{n_m}} \frac{1}{2} \sum_{i=1}^{n_m}\sum_{j:r(i,j)=1} ((\theta^{j})^Tx^{i}-y^{(i,j)})^2
$$

但是如果我们既没有用户的参数，也没有电影的特征该怎么办呢？其实这也是工业界研究的比较多的关于推荐系统的问题，也叫冷启动问题。

名字非常的形象，刚刚启动的系统处于一个“冷”状态，意味着数据量小，在不断的积累数据的过程中，算法的表现也越来越好，系统逐渐“手感火热”

而对于协同过滤算法，它的基本思想是这样的
- 我们会初始化$x^1,x^2,...,x^{n_m},\theta^1,...,\theta^{n_u}$为一些比较小的值，这样在冷启动时为用户的推荐就会比较随机
- 修改优化目标，结合求用户参数和电影特征的代价函数，让新的代价函数同时对$x$和$\theta$进行，并使用梯度下降法进行训练
- 训练完算法后，我们会对用户$j$对电影$i$的评分做出预测

在实际运用中，我们还可以先固定$x^1,...,x^{n_m}$，然后对$\theta$做出预测，然后再让$\theta^1,...,\theta^{n_u}$固定，估计$x^1,...,x^{n_m}$，这有点像Google的PageRank算法和神经网络中的反向传播算法

## 总结

基于内容的推荐只考虑了对象本身的性质，提取出对象的特征，然后来和用户参数进行计算。

而基于协同过滤的推荐算法考虑到了全部的数据，推荐的个性化程度比较高一些

你可以想象，让$\theta^1,...,\theta^{n_u}$固定时，我们其实是在考虑用户的历史习惯，固定$x^1,...,x^{n_m}$时，我们其实是在考虑兴趣相近的用户可能会对同样的东西感兴趣。这也是协同过滤的假设。


[推荐系统](https://zh.wikipedia.org/wiki/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1)
[协同过滤算法](https://zh.wikipedia.org/wiki/%E5%8D%94%E5%90%8C%E9%81%8E%E6%BF%BE)
[PageRank算法](https://zh.wikipedia.org/zh-hans/PageRank)
