---
layout:     post
title:      CS229-正则化与模型选择
subtitle:   机器学习
date:       2019-12-16
author:     Pkun
header-img: img/stfml.jpg
catalog: true
tags:
    - 机器学习
---


大部分内容来自CS229吴恩达老师的课程讲义，感谢翻译

| 原作者 | 翻译 | 校对 |
| --- | --- | --- |
| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |


|相关链接|
|---|
|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|
|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|
|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|
|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|

# 正则化与模型选择

设想一个机器学习的问题，我们要从一系列不同的模型中进行挑选。例如，我们可能是用一个多项式回归模型  $h_\theta (x)=g(\theta_0+\theta_1x+\theta_2x^2+\cdots+\theta_kx^k)$ ，想要判定这里的多项式次数 $k$ 应该是多少。那么我们怎么才能自动选择一个能够在偏差 (bias)/方差(variance)$^1$之间进行权衡的模型呢?

## 交叉验证

### 保留交叉验证

保留交叉验证 (hold-out cross validation)，也叫简单交叉验证 (simple cross validation)，步骤如下：

1. 随机拆分训练集 $S$ 成 $S_{train}$ (例如，可以选择整体数据中 的 70% 用于训练) 和 $S_{cv}$ (训练集中剩余的 30%用于验 证)。这里的 $S_{cv}$ 就叫做保留交叉验证集。 
2. 只对集合 $S_{train}$ 中的每一个模型 $M_i$ 进行训练，然后得到假设类 $h_i$。 
3. 筛选并输出对保留交叉验证集有最小误差 $\hat\epsilon_{S_{cv}}(h_i)$ 的假设$h_i$ 。(回忆一下，这里的 $\hat\epsilon_{S_{cv}}(h_i)$ 表示的是假设 $h$ 在保留交叉验证集 $S_{cv}$ 中的样本的经验误差。) 

这样通过在一部分未进行训练的样本集合 $S_{cv}$ 上进行测试，我们对每个假设 $h_i$ 的真实泛化误差就能估计的更好，然后就能选择出来一个有最小估计泛化误差的假设了。通常可以选择 1/4 到 1/3 的数据样本用来作为保留交叉验证集，30% 是一个很典型的选择。

还有另外一种备选方法，就是在第三步的时候，也可以换做选 择与最小估计经验误差 $\hat\epsilon_{S_{cv}}(h_i)$ 对应的模型 $M_i$ ，然后对整个训练样本数据集 $S$ 使用 $M_i$ 来进行再次训练。(这个思路通常都不错，但有一种情景例外，就是学习算法对初始条件和数据的扰动非常敏感的情况。在这样的方法中，适用于 $S_{train}$ 的模型未必就能够同样适用于 $S_{cv}$，这样就最好还是放弃再训练的步骤。) 

### k-折交叉验证

使用保留交叉验证集的一个弊端就是“浪费(waste)”了训练样本数据集的 30% 左右。甚至即便我们使用了备选的那个针对整个训练集使用模型进行重新训练的步骤，也还不成，因为这无非是相当于我们只尝试在一个 $0.7m$ 规模的训练样本集上试图寻找一个好的模型来解决一个机器学习问题，而并不是使用了全部的 $m$ 个训练样 本，因为我们进行测试的都是每次在仅 $0.7m$ 规模样本上进 行训练而得到的模型。当然了，如果数据非常充足，或者是很廉价的话，也可以用这种方法，而**如果训练样本数据本身就很 稀缺的话（例如说只有 20 个样本），那就最好用其他方法了。** 

下面就是一种这样的方法，名字叫 **k-折交叉验证**，这样每次的用于验证的保留数据规模都更小:  

1. 随机将训练集 $S$ 切分成 $k$ 个不相交的子集。其中每一个子集的规模为 $m/k$ 个训练样本。这些子集为 $S_1,\cdots,S_k$

2. 对每个模型 $M_i$，我们都按照下面的步骤进行评估(evaluate):

   对 $j=1,\cdots,k$ 

   - 在 $S_1\cup\cdots\cup S_{j-1}\cup S_{j+1}\cup\cdots\cup S_k$ (也就是除了 $S_j$ 之外的其他数据)，对模型 $M_i$ 得到假设 $h_{ij}$ 。接下来针对 $S_j$ 使用假设 $h_{ij}$ 进行测试，得到经验误差 $\hat\epsilon_{S_{cv}}(h_{ij})$ 

     对$\hat\epsilon_{S_{cv}}(h_{ij})$ 取平均值，计算得到的值就当作是模型 $M_i$ 的估计泛化误差

3. 选择具有最小估计泛化误差的模型 $M_i$ 的，然后在整个训练样本集 $S$ 上重新训练该模型。这样得到的假设 (hypothesis)就可以输出作为最终结果了。  

通常这里进行折叠的次数$k$ 一般是 10，即 $k = 10$。这样每次进行保留用于验证的数据块就只有 $1/k$ ，这 就比之前的 30% 要小多了，当然这样一来这个过程也要比简单的保留交叉验证方法消耗更多算力成本，因为现在需要对每个模型都进行 $k$ 次训练。 

虽然通常选择都是设置 $k = 10$，不过如果一些问题中数据量 确实很匮乏，那有时候也可以走一点极端，设 $k = m$，由于每次都保留了一个训练样本，所以这个方法就叫做**留一法交叉验证**。

这 种情况下，我们需要在训练样本集 $S$ 中除了某一个样本外的其他所有样本上进行训练，然后在保留出来的单独样本上进行检验。然后把计算出来的 $m = k$个误差放到一起求平均值， 这样就得到了对一个模型的泛化误差的估计。

## 特征选择

模型选择的一个非常重要的特殊情况就是特征选择。在一些问题中，我们可能有一大堆数据，但是真正和学习任务相关的特别少。甚至即便是针对 $n$ 个输入特征值使用一个简单的线性分类器，你的假设类的 $VC$ 维 也依然能达到 $O(n)$，因此有过拟合 的潜在风险，除非训练样本集也足够巨大。 

### 向前/先后搜索

在这样的一个背景下，你就可以使用一个特征选择算法，来降低特征值的数目。假设有 $n$ 个特征，那么就有 $2^n$ 种可能的特征子集 (因为 $n$ 个特征中的任意一个都可以被某个特征子集，因此特征选择就可以看做是一个对 $2^n$ 种可能的模型进行选择。对于特别大的 $n$，要是彻底枚举(enumerate)和对比全部 $2^n$ 种模型，成本就太高了， 所以通常的做法都是使用某些启发式的搜索过程来找到一个好的特征子集。下面的搜索过程叫做**向前搜索(forward search) ：**

1. 初始化一个集合为空集 $\mathcal F=\emptyset$

2. 循环下面的过程{

   (a) 对于 $i=1,\cdots,n$ 如果 $i\notin \mathcal F$，则令 $\mathcal F_i=\mathcal F\cup \{i\}$，然后使用某种交叉验证来评估特征 $\mathcal F_i$ 

   (b) 令 $\mathcal F$ 为(a)中最佳特征子集

   }

3. 整个搜索过程中筛选出来了最佳特征子集(best feature subset)，将其输出。 

算法的外层循环可以在 $\mathcal F=\{1,\cdots,n\}$ 达到全部特征规模时停止，也可以在 $|\mathcal F|$ 超过某个预先设定的阈值时停止（阈值和你想要算法用到特征数量最大值有关）。

这个算法描述的是对模型特征选择进行包装(**包装器特征选择，Wrapper feature selection** )的一个实例，此算法本身就是一个将学习算法进行“打包(wraps)”的过程，然后重复调用这个学习算法来评估此算法对不同的特征子集的处理效果。除了向前搜索外，还可以使用其他的搜索过程。例如，可以**逆向搜索(backward search)**，从$\mathcal F = \{1, ..., n\}$ ，即规模等同于全部特征开始，然后重复，每次删减一个特征，直到 $\mathcal F$ 为空集时终止。 

这种包装器特征选择算法通常效果不错，不过对算力开销也很大，尤其是要对学习算法进行多次调用。实际上，完整的向前搜索将要对学习算法调用约 $O(n^2)$ 次。 

### 过滤器特征选择

**过滤器特征选择(Filter feature selection methods)** 给出的特征子集选择方法更具有启发性(heuristic)，而且在算力上的开销成本也更低。这里的一个思路是，计算一个简单的分数 $S(i)$，用来衡量每个特征 $x_i$ 对分类标签(class labels) $y$ 所能体现的信息量。然后，只需根据需要选择最大分数 $S(i)$ 的 $k$ 个特征。 

怎么去定义用于衡量信息量的分值 $S(i)$ 呢?一种思路是使用 $x_i$ 和 $y$ 之间的相关系数的值，这可以在训练样本数据中算出。这样我们选出的就是与分类标签(class labels)的关系最密切的特征值(features)。实践中，通常（尤其当特征 $x_i$ 为离散值）选择 $x_i$ 和 $y$ 的**互信息( mutual information, ${\rm{MI}}(x_i, y)$ )** 来作为 $S(i)$ 。 

$$
{\rm{MI}}(x_i, y)=\sum_{x_i\in\{0, 1\}}\sum_{y\in\{0,1\}}p(x_i,y)\log\frac{p(x_i,y)}{p(x_i)p(y)}
$$

(上面这个等式假设了 $x_i$ 和 $y$ 都是二值化；更广泛的情况下将会超过变量的范围 。)上式中的概率$p(x_i,y)$，$p(x_i)$ 和 $p(y)$ 都可以根据它们在训练集上的经验分布(empirical distributions)而推测(estimated)得到。 

要对这个信息量分值的作用有一个更直观的印象，可以将互信息表达成 $KL$ 散度(Kullback-Leibler divergence，也称 $KL$ 距离，常用来衡量两个概率分布的距离): 

$$
{\rm{MI}}(x_i,y)={\rm KL}(p(x_i,y)\,\|\,p(x_i)p(y))
$$

比较通俗地说，这个概念对 $p(x_i,y)$ 和 $p(x_i)p(y)$ 的概率分布的差异程度给出一个衡量。如果 $x_i$ 和 $y$ 是两个独立的随机变量，那么必然有 $p(x_i, y) = p(x_i)p(y)$，而两个分布之间的 $KL$ 散度就应该是 $0$。

这也符合下面这种很自然的认识：如果 $x_i$ 和 $y$ 相互独立，那么 $x_i$ 很明显对 $y$ 是“完全无信息量”(non-informative)，因此对应的信息量分值 $S(i)$ 就应该很小。与之相反地，如果 $x_i$ 对 $y$ “有很大的信息量 (informative)”，那么这两者的互信息 ${\rm MI}(x_i,y)$ 就应该很大。  

最后一个细节：现在你已经根据信息量分值 $S(i)$ 的高低来对特征组合进行了排序，那么要如何选择特征个数 $k$ 呢?一个标准办法就是使用交叉验证(cross validation)来从可能的不同 $k$ 值中进行筛选。例如，在对文本分类(text classification)使用朴素贝叶斯方法(naive Bayes)，这个问题中的词汇规模(vocabulary size) $n$ 通常都会特别大，使用交叉验证的方法来选择特征子集(feature subset)，一般都提高分类器精度。 

