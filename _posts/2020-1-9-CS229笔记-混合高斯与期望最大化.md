---
layout:     post
title:      CS229-混合高斯与期望最大化
subtitle:   机器学习
date:       2020-1-9
author:     Pkun
header-img: img/stfml.jpg
catalog: true
tags:
    - 机器学习
---


大部分内容来自CS229吴恩达老师的课程讲义，感谢翻译

| 原作者 | 翻译 | 校对 |
| --- | --- | --- |
| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |


|相关链接|
|---|
|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|
|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|
|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|
|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|


## 混合高斯 (Mixtures of Gaussians) 和期望最大化算法(the EM algorithm)

这里的内容主要是使用EM算法进行密度估计

### 开始之前

假如我们有一个训练集$\{x^{(1)},...,x^{(m)}\}$，因为是无监督学习，所以每个样本没有标签。

现在我们希望能够获得一个联合分布$p(x^{(i)},z^{(i)}) = p(x^{(i)} \mid z^{(i)})p(z^{(i)})$ 来对数据进行建模。其中的 $z^{(i)} \sim Multinomial(\phi)$ （即$z^{(i)}$ 是一个以 $\phi$ 为参数的多项式分布，其中 $\phi_j \ge 0, \sum_{j=1}^k \phi_j=1$，而参数 $\phi_j$ 给出了 $p(z^{(i)} = j)$），另外 $x^{(i)} \mid z^{(i)} = j \sim N(μ_j,\Sigma_j)$ **（译者注：$x^{(i)} \mid z^{(i)} = j$是一个以 $μ_j$ 和 $\Sigma_j$ 为参数的正态分布）**。我们设 $k$ 来表示 $z^{(i)}$ 能取的值的个数。因此，我们这个模型就是在假设每个$x^{(i)}$ 都是从$\{1, ..., k\}$中随机选取$z^{(i)}$来生成的，然后 $x^{(i)}$ 就是服从$k$个高斯分布中的一个，而这$k$个高斯分布又取决于$z^{(i)}$。这就叫做一个**混合高斯模型。** 此外还要注意的就是这里的 $z^{(i)}$ 是**潜在的**随机变量，这就意味着其取值可能还是隐藏的或者未被观测到的。这就会增加这个估计问题的难度。

### 混合高斯

对于高斯模型，我们有参数$\phi, \mu$ 和 $\Sigma$，然后为了对这些参数进行估计，我们写出似然函数
$$
\begin{aligned}
l(\phi,\mu,\Sigma) &= \sum_{i=1}^m \log p(x^{(i)};\phi,\mu,\Sigma) \\
&= \sum_{i=1}^m \log \sum_{z^{(i)}=1}^k p(x^{(i)} \mid z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
\end{aligned}
$$

接着，我们设上面方程的导数为0来尝试求解，然后就会发现不可能以闭合形式来找到这些参数的最大似然估计。

这主要是因为我们没有观测到$z^{(i)}$，如果我们已知 $z^{(i)}$，这个最大似然估计问题就简单很多了。这样就可以把似然函数写成下面这种形式
$$
l(\phi,\mu,\Sigma)=\sum_{i=1}^m \log p(x^{(i)} \mid z^{(i)};\mu,\Sigma) + \log p(z^{(i)};\phi)
$$

对上面的函数进行最大化，就能得到对应的参数$\phi, \mu$ 和 $\Sigma$：

$$
\begin{aligned}&\phi_j=\frac 1m\sum_{i=1}^m 1\{z^{(i)}=j\}, \\&\mu_j=\frac{\sum_{i=1}^m 1\{z^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{z^{(i)}=j\}}, \\&\Sigma_j=\frac{\sum_{i=1}^m 1\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m 1\{z^{(i)}=j\}}.\end{aligned}
$$

如果 $z^{(i)}$ 是已知的，那么这个最大似然估计就几乎等同于之前用高斯判别分析模型中对参数进行的估计，唯一不同在于这里的 $z^{(i)}$ 扮演了高斯判别分析当中的分类标签的角色。

>这里的式子和之前在 PS1 中高斯判别分析的方程还有一些小的区别，这首先是因为在此处我们把 $z^{(i)}$ 泛化为多项式分布（multinomial），而不是伯努利分布（Bernoulli），其次是由于这里针对高斯分布中的每一项使用了一个不同的 $\Sigma_j$。

但是在密度估计问题里面，我们是不知道$z^{(i)}$的，这就要用到**期望最大化算法**了

### 期望最大化算法 上

期望最大化算法（EM，Expectation-Maximization）是一个迭代算法，有两个主要的步骤。针对我们这个问题，在 $E$ 这一步中，程序是试图去“猜测（guess）” $z^{(i)}$ 的值。然后在 $M$ 这一步，就根据上一步的猜测来对模型参数进行更新。由于在 $M$ 这一步当中我们假设（pretend）了上一步是对的，那么最大化的过程就简单了。下面是这个算法：


&emsp;重复下列过程直到收敛（convergence）: {

&emsp;&emsp;（$E$-步骤）对每个 $i, j$, 设 

$$
w_j^{(i)} := p(z^{(i)}=j \mid x^{(i)};\phi,\mu,\Sigma)
$$

&emsp;&emsp;（$M$-步骤）更新参数：

$$
\begin{aligned}&\phi_j=\frac 1m\sum_{i=1}^m w_j^{(i)}, \\&\mu_j=\frac{\sum_{i=1}^m w_j^{(i)}x^{(i)}}{\sum_{i=1}^m w_j^{(i)}}, \\&\Sigma_j=\frac{\sum_{i=1}^m w_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m w_j^{(i)}}.\end{aligned}
$$
} 

在 $E$ 步骤中，在给定 $x^{(i)}$ 以及使用当前参数设置情况下，我们计算出了参数 $z^{(i)}$ 的后验概率。使用贝叶斯公式，就得到下面的式子：

$$
p(z^{(i)}=j \mid x^{(i)};\phi,\mu,\Sigma)=
\frac{p(x^{(i)} \mid z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}
{\sum_{l=1}^k p(x^{(i)} \mid z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}
$$

上面的式子中，$p(z^{(i)}=j \mid x^{(i)};\phi,\mu,\Sigma)$ 是通过评估一个高斯分布的密度得到的，这个高斯分布的均值为 $\mu_i$，对$x^{(i)}$的协方差为$\Sigma_j$；$p(z^{(i)} = j;\phi)$ 是通过 $\phi_j$ 得到，以此类推。在 $E$ 步骤中计算出来的 $w_j^{(i)}$ 代表了我们对 $z^{(i)}$ 这个值的“弱估计”$^2$。

>2 这里用的词汇“弱（soft）”是指我们对概率进行猜测，从 $[0, 1]$ 这样一个闭区间进行取值；而与之对应的“强（hard）”值得是单次最佳猜测，例如从集合 $\{0,1\}$ 或者 $\{1, ..., k\}$ 中取一个值。

>3 来自pkun的说明：这里之所以用$w^{(i)}_j$而不是采用类似的$\phi_j=\frac 1m\sum_{i=1}^m 1\{z^{(i)}=j\}$，具体原因和上面的强弱估计差不多，主要还是因为后者不是估计，而是直接根据你猜测的结果计算出来了一个数，然后你把他当成正确的概率，前者才是估计，使用贝叶斯公式计算出来的概率

另外在 $M$ 步骤中进行的更新还要与 $z^{(i)}$ 已知之后的方程式进行对比。它们是相同的，不同之处只在于之前使用的指示函数（indicator functions），指示每个数据点所属的高斯分布，而这里换成了 $w_j^{(i)}$。


很明显，$EM$ 算法对 $z^{(i)}$ 进行重复的猜测，这种思路很自然；但这个算法是怎么产生的，以及我们能否确保这个算法的某些特性，例如收敛性之类的？我们会讲解一种对 $EM$ 算法更泛化的解读，这样我们就可以在其他的估计问题中轻松地使用 $EM$ 算法了，只要这些问题也具有潜在变量，并且还能够保证收敛。

### Jensen不等式

如果函数 $f$ 的二阶导数 $f''(x) \ge 0$ （其中的 $x \in R$），则函数 $f$ 为一个凸函数（convex function）。

如果输入的为向量变量，那么这个函数就泛化了，这时候该函数的海森矩阵（hessian） $H$ 就是一个半正定矩阵。如果对于所有的 $x$ ，都有二阶导数 $f''(x) > 0$，那么我们称这个函数 $f$ 是严格凸函数（对应向量值作为变量的情况，对应的条件就是海森矩阵必须为正定，写作 $H > 0$）

定理（Theorem）：** 设 $f$ 是一个凸函数，且设 $X$ 是一个随机变量（random variable）。然后则有：

$$
E[f(X)] \ge f(EX).
$$

此外，如果函数 $f$ 是严格凸函数，那么 $E[f(X)] = f(EX)$ 当且仅当 $X = E[X]$ 的概率（probability）为 $1$ 的时候成立（例如 $X$ 是一个常数）。


### 期望最大化算法 下

我们用模型 $p(x, z)$ 对数据进行建模，拟合其参数（parameters），其中的似然函数（likelihood）如下所示：

$$
\begin{aligned}l(\theta) &= \sum_{i=1}^m\log p(x;\theta) \\&= \sum_{i=1}^m\log\sum_z p(x,z;\theta)\end{aligned}
$$


使用期望最大化算法就能很有效地实现最大似然估计。明确地对似然函数$l(\theta)$进行最大化可能是很困难的，所以我们的策略就是使用一种替代，在 $E$ 步骤 构建一个 $l$ 的下限，然后在 $M$ 步骤 对这个下限进行优化。

对于每个 $i$，设 $Q_i$ 是某个对 $z$ 的分布（$\sum_z Q_i(z) = 1, Q_i(z)\ge 0$）。则有下列各式$^1$：

$$
\begin{aligned}
\sum_i\log p(x^{(i)};\theta) &= \sum_i\log\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)&(1) \\
&= \sum_i\log\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} &(2)\\
&\ge \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}&(3) 
\end{aligned}
$$

>1 如果 $z$ 是连续的，那么 $Q_i$ 就是一个密度函数，上面讨论中提到的对 $z$ 的求和就要用对 $z$ 的积分来替代。

上面推导的最后一步使用了 Jensen 不等式。其中的 $f(x) = log x$ 是一个凹函数（concave function），因为其二阶导数 $f''(x) = -1/x^2 < 0$ 在整个定义域（domain） $x\in R^+$ 上都成立。

此外，上式的求和中的单项： 

$$
\sum_{z^{(i)}}Q_i(z^{(i)})[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]
$$

是变量$[p(x^{(i)}, z^{(i)}; \theta)/Q_i(z^{(i)})]$ 基于 $z^{(i)}$ 的期望，其中 $z^{(i)}$ 是根据 $Q_i$ 给定的分布确定。然后利用 Jensen 不等式，就得到了：

$$
f(E_{z^{(i)}\sim Q_i}[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}])\ge
E_{z^{(i)}\sim Q_i}[f(\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})]
$$

其中上面的角标 $“z^{(i)}\sim Q_i”$ 就表明这个期望是对于依据分布 $Q_i$ 来确定的 $z^{(i)}$ 的。这样就可以从等式 $(2)$ 推导出等式 $(3)$。

接下来，对于任意的一个分布 $Q_i$，上面的等式 $(3)$ 就给出了似然函数 $l(\theta)$ 的下限。那么对于 $Q_i$ 有很多种选择。咱们该选哪个呢？如果我们对参数 $\theta$ 有某种当前的估计，很自然就可以设置这个下限为 $\theta$ 这个值。也就是，针对当前的 $\theta$ 值，我们令上面的不等式中的符号为等号。（稍后我们能看到，这样就能证明，随着 $EM$迭代过程的进行，似然函数 $l(\theta)$ 就会单调递增。）

为了让上面的限定值与 $\theta$ 特定值联系紧密，我们需要上面推导过程中的 Jensen 不等式这一步中等号成立。要让这个条件成立，我们只需确保是在对一个常数值随机变量求期望。也就是需要：

$$
\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c
$$

其中常数 $c$ 不依赖 $z^{(i)}$。要实现这一条件，只需满足：

$$
Q_i(z^{(i)})\propto p(x^{(i)},z^{(i)};\theta)
$$

实际上，由于我们已知 $\sum_z Q_i(z^{(i)}) = 1$（因为这是一个分布），这就进一步表明：

$$
\begin{aligned}
Q_i(z^{(i)}) &= \frac{p(x^{(i)},z^{(i)};\theta)}{\sum_z p(x^{(i)},z;\theta)} \\
&= \frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} \\
&= p(z^{(i)} \mid x^{(i)};\theta)
\end{aligned}
$$

因此，在给定 $x^{(i)}$ 和参数 $\theta$ 的设置下，我们可以简单地把 $Q_i$ 设置为 $z^{(i)}$ 的后验分布。

（未完待续）