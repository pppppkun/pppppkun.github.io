---
layout:     post
title:      CS229-混合高斯与期望最大化
subtitle:   机器学习
date:       2020-01-09
author:     Pkun
header-img: img/stfml.jpg
catalog: true
tags:
    - 机器学习
---


大部分内容来自CS229吴恩达老师的课程讲义，感谢翻译

| 原作者 | 翻译 | 校对 |
| --- | --- | --- |
| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |


|相关链接|
|---|
|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|
|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|
|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|
|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|


## 混合高斯 (Mixtures of Gaussians) 和期望最大化算法(the EM algorithm)

这里的内容主要是使用EM算法进行密度估计

### 开始之前

假如我们有一个训练集$\{x^{(1)},...,x^{(m)}\}$，因为是无监督学习，所以每个样本没有标签。

现在我们希望能够获得一个联合分布$p(x^{(i)},z^{(i)}) = p(x^{(i)} \mid z^{(i)})p(z^{(i)})$ 来对数据进行建模。其中的 $z^{(i)} \sim Multinomial(\phi)$ （即$z^{(i)}$ 是一个以 $\phi$ 为参数的多项式分布，其中 $\phi_j \ge 0, \sum_{j=1}^k \phi_j=1$，而参数 $\phi_j$ 给出了 $p(z^{(i)} = j)$），另外 $x^{(i)} \mid z^{(i)} = j \sim N(μ_j,\Sigma_j)$ **（译者注：$x^{(i)} \mid z^{(i)} = j$是一个以 $μ_j$ 和 $\Sigma_j$ 为参数的正态分布）**。我们设 $k$ 来表示 $z^{(i)}$ 能取的值的个数。因此，我们这个模型就是在假设每个$x^{(i)}$ 都是从$\{1, ..., k\}$中随机选取$z^{(i)}$来生成的，然后 $x^{(i)}$ 就是服从$k$个高斯分布中的一个，而这$k$个高斯分布又取决于$z^{(i)}$。这就叫做一个**混合高斯模型。** 此外还要注意的就是这里的 $z^{(i)}$ 是**潜在的**随机变量，这就意味着其取值可能还是隐藏的或者未被观测到的。这就会增加这个估计问题的难度。

### 混合高斯

对于高斯模型，我们有参数$\phi, \mu$ 和 $\Sigma$，然后为了对这些参数进行估计，我们写出似然函数
$$
\begin{aligned}
l(\phi,\mu,\Sigma) &= \sum_{i=1}^m \log p(x^{(i)};\phi,\mu,\Sigma) \\
&= \sum_{i=1}^m \log \sum_{z^{(i)}=1}^k p(x^{(i)} \mid z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
\end{aligned}
$$

接着，我们设上面方程的导数为0来尝试求解，然后就会发现不可能以闭合形式来找到这些参数的最大似然估计。

这主要是因为我们没有观测到$z^{(i)}$，如果我们已知 $z^{(i)}$，这个最大似然估计问题就简单很多了。这样就可以把似然函数写成下面这种形式
$$
l(\phi,\mu,\Sigma)=\sum_{i=1}^m \log p(x^{(i)} \mid z^{(i)};\mu,\Sigma) + \log p(z^{(i)};\phi)
$$

对上面的函数进行最大化，就能得到对应的参数$\phi, \mu$ 和 $\Sigma$：

$$
\begin{aligned}&\phi_j=\frac 1m\sum_{i=1}^m 1\{z^{(i)}=j\}, \\&\mu_j=\frac{\sum_{i=1}^m 1\{z^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{z^{(i)}=j\}}, \\&\Sigma_j=\frac{\sum_{i=1}^m 1\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m 1\{z^{(i)}=j\}}.\end{aligned}
$$

如果 $z^{(i)}$ 是已知的，那么这个最大似然估计就几乎等同于之前用高斯判别分析模型中对参数进行的估计，唯一不同在于这里的 $z^{(i)}$ 扮演了高斯判别分析当中的分类标签的角色。

>这里的式子和之前在 PS1 中高斯判别分析的方程还有一些小的区别，这首先是因为在此处我们把 $z^{(i)}$ 泛化为多项式分布（multinomial），而不是伯努利分布（Bernoulli），其次是由于这里针对高斯分布中的每一项使用了一个不同的 $\Sigma_j$。

但是在密度估计问题里面，我们是不知道$z^{(i)}$的，这就要用到**期望最大化算法**了

### 期望最大化算法 上

期望最大化算法（EM，Expectation-Maximization）是一个迭代算法，有两个主要的步骤。针对我们这个问题，在 $E$ 这一步中，程序是试图去“猜测（guess）” $z^{(i)}$ 的值。然后在 $M$ 这一步，就根据上一步的猜测来对模型参数进行更新。由于在 $M$ 这一步当中我们假设（pretend）了上一步是对的，那么最大化的过程就简单了。下面是这个算法：


&emsp;重复下列过程直到收敛（convergence）: {

&emsp;&emsp;（$E$-步骤）对每个 $i, j$, 设 

$$
w_j^{(i)} := p(z^{(i)}=j \mid x^{(i)};\phi,\mu,\Sigma)
$$

&emsp;&emsp;（$M$-步骤）更新参数：

$$
\begin{aligned}&\phi_j=\frac 1m\sum_{i=1}^m w_j^{(i)}, \\&\mu_j=\frac{\sum_{i=1}^m w_j^{(i)}x^{(i)}}{\sum_{i=1}^m w_j^{(i)}}, \\&\Sigma_j=\frac{\sum_{i=1}^m w_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m w_j^{(i)}}.\end{aligned}
$$

} 

在 $E$ 步骤中，在给定 $x^{(i)}$ 以及使用当前参数设置情况下，我们计算出了参数 $z^{(i)}$ 的后验概率。使用贝叶斯公式，就得到下面的式子：

$$
p(z^{(i)}=j \mid x^{(i)};\phi,\mu,\Sigma)=
\frac{p(x^{(i)} \mid z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}
{\sum_{l=1}^k p(x^{(i)} \mid z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}
$$

上面的式子中，$p(z^{(i)}=j \mid x^{(i)};\phi,\mu,\Sigma)$ 是通过评估一个高斯分布的密度得到的，这个高斯分布的均值为 $\mu_i$，对$x^{(i)}$的协方差为$\Sigma_j$；$p(z^{(i)} = j;\phi)$ 是通过 $\phi_j$ 得到，以此类推。在 $E$ 步骤中计算出来的 $w_j^{(i)}$ 代表了我们对 $z^{(i)}$ 这个值的“弱估计”$^2$。

>2 这里用的词汇“弱（soft）”是指我们对概率进行猜测，从 $[0, 1]$ 这样一个闭区间进行取值；而与之对应的“强（hard）”值得是单次最佳猜测，例如从集合 $\{0,1\}$ 或者 $\{1, ..., k\}$ 中取一个值。

>3 来自pkun的说明：这里之所以用$w^{(i)}_j$而不是采用类似的$\phi_j=\frac 1m\sum_{i=1}^m 1\{z^{(i)}=j\}$，具体原因和上面的强弱估计差不多，主要还是因为后者不是估计，而是直接根据你猜测的结果计算出来了一个数，然后你把他当成正确的概率，前者才是估计，使用贝叶斯公式计算出来的概率

另外在 $M$ 步骤中进行的更新还要与 $z^{(i)}$ 已知之后的方程式进行对比。它们是相同的，不同之处只在于之前使用的指示函数（indicator functions），指示每个数据点所属的高斯分布，而这里换成了 $w_j^{(i)}$。


很明显，$EM$ 算法对 $z^{(i)}$ 进行重复的猜测，这种思路很自然；但这个算法是怎么产生的，以及我们能否确保这个算法的某些特性，例如收敛性之类的？我们会讲解一种对 $EM$ 算法更泛化的解读，这样我们就可以在其他的估计问题中轻松地使用 $EM$ 算法了，只要这些问题也具有潜在变量，并且还能够保证收敛。

### Jensen不等式

如果函数 $f$ 的二阶导数 $f''(x) \ge 0$ （其中的 $x \in R$），则函数 $f$ 为一个凸函数（convex function）。

如果输入的为向量变量，那么这个函数就泛化了，这时候该函数的海森矩阵（hessian） $H$ 就是一个半正定矩阵。如果对于所有的 $x$ ，都有二阶导数 $f''(x) > 0$，那么我们称这个函数 $f$ 是严格凸函数（对应向量值作为变量的情况，对应的条件就是海森矩阵必须为正定，写作 $H > 0$）

定理（Theorem）：** 设 $f$ 是一个凸函数，且设 $X$ 是一个随机变量（random variable）。然后则有：

$$
E[f(X)] \ge f(EX).
$$

此外，如果函数 $f$ 是严格凸函数，那么 $E[f(X)] = f(EX)$ 当且仅当 $X = E[X]$ 的概率（probability）为 $1$ 的时候成立（例如 $X$ 是一个常数）。


### 期望最大化算法 下

我们用模型 $p(x, z)$ 对数据进行建模，拟合其参数（parameters），其中的似然函数（likelihood）如下所示：

$$
\begin{aligned}l(\theta) &= \sum_{i=1}^m\log p(x;\theta) \\&= \sum_{i=1}^m\log\sum_z p(x,z;\theta)\end{aligned}
$$


使用期望最大化算法就能很有效地实现最大似然估计。明确地对似然函数$l(\theta)$进行最大化可能是很困难的，所以我们的策略就是使用一种替代，在 $E$ 步骤 构建一个 $l$ 的下限，然后在 $M$ 步骤 对这个下限进行优化。

对于每个 $i$，设 $Q_i$ 是某个对 $z$ 的分布（$\sum_z Q_i(z) = 1, Q_i(z)\ge 0$）。则有下列各式$^1$：

$$
\begin{aligned}
\sum_i\log p(x^{(i)};\theta) &= \sum_i\log\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)&(1) \\
&= \sum_i\log\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} &(2)\\
&\ge \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}&(3) 
\end{aligned}
$$

>1 如果 $z$ 是连续的，那么 $Q_i$ 就是一个密度函数，上面讨论中提到的对 $z$ 的求和就要用对 $z$ 的积分来替代。

上面推导的最后一步使用了 Jensen 不等式。其中的 $f(x) = log x$ 是一个凹函数（concave function），因为其二阶导数 $f''(x) = -1/x^2 < 0$ 在整个定义域（domain） $x\in R^+$ 上都成立。

此外，上式的求和中的单项： 

$$
\sum_{z^{(i)}}Q_i(z^{(i)})[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]
$$

是变量$[p(x^{(i)}, z^{(i)}; \theta)/Q_i(z^{(i)})]$ 基于 $z^{(i)}$ 的期望，其中 $z^{(i)}$ 是根据 $Q_i$ 给定的分布确定。然后利用 Jensen 不等式，就得到了：

$$
f(E_{z^{(i)}\sim Q_i}[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}])\ge
E_{z^{(i)}\sim Q_i}[f(\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})]
$$

其中上面的角标 $“z^{(i)}\sim Q_i”$ 就表明这个期望是对于依据分布 $Q_i$ 来确定的 $z^{(i)}$ 的。这样就可以从等式 $(2)$ 推导出等式 $(3)$。

接下来，对于任意的一个分布 $Q_i$，上面的等式 $(3)$ 就给出了似然函数 $l(\theta)$ 的下限。那么对于 $Q_i$ 有很多种选择。咱们该选哪个呢？如果我们对参数 $\theta$ 有某种当前的估计，很自然就可以设置这个下限为 $\theta$ 这个值。也就是，针对当前的 $\theta$ 值，我们令上面的不等式中的符号为等号。（稍后我们能看到，这样就能证明，随着 $EM$迭代过程的进行，似然函数 $l(\theta)$ 就会单调递增。）

为了让上面的限定值与 $\theta$ 特定值联系紧密，我们需要上面推导过程中的 Jensen 不等式这一步中等号成立。要让这个条件成立，我们只需确保是在对一个常数值随机变量求期望。也就是需要：

$$
\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c
$$

其中常数 $c$ 不依赖 $z^{(i)}$。要实现这一条件，只需满足：

$$
Q_i(z^{(i)})\propto p(x^{(i)},z^{(i)};\theta)
$$

实际上，由于我们已知 $\sum_z Q_i(z^{(i)}) = 1$（因为这是一个分布），这就进一步表明：

$$
\begin{aligned}
Q_i(z^{(i)}) &= \frac{p(x^{(i)},z^{(i)};\theta)}{\sum_z p(x^{(i)},z;\theta)} \\
&= \frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} \\
&= p(z^{(i)} \mid x^{(i)};\theta)
\end{aligned}
$$

因此，在给定 $x^{(i)}$ 和参数 $\theta$ 的设置下，我们可以简单地把 $Q_i$ 设置为 $z^{(i)}$ 的后验分布。

接下来，对 $Q_i$ 的选择，等式 $(3)$ 就给出了似然函数对数的一个下限，而似然函数正是我们要试图求的最大值，于是我们让等号可以取到，这就是 $E$ 步骤。接下来在算法的 $M$ 步骤中，就最大化等式 $(3)$ 当中的方程，然后得到新的参数 $\theta$。重复这两个步骤，就是完整的 $EM$ 算法，如下所示：

&emsp;重复下列过程直到收敛（convergence）:  {

&emsp;&emsp;（$E$ 步骤）对每个 $i$，设 
　　
$$
Q_i(z^{(i)}):=p(z^{(i)} \mid x^{(i)};\theta)
$$

&emsp;&emsp;（$M$ 步骤） 设 

$$
\theta := arg\max_\theta\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

} 

怎么才能知道这个算法是否会收敛呢？设 $\theta^{(t)}$ 和 $\theta^{(t+1)}$ 是上面 $EM$ 迭代过程中的某两个参数。接下来我们可以证明 $l(\theta^{(t)})\le l(\theta^{(t+1)})$，这就表明 $EM$ 迭代过程总是让似然函数对数单调递增。证明这个结论的关键就在于对 $Q_i$ 的选择中。在上面$EM$迭代中，参数的起点设为 $\theta^{(t)}$，我们就可以选择 $Q_i^{(t)}(z^{(i)}): = p(z^{(i)} \mid x^{(i)};\theta^{(t)})$，这样选择能保证 Jensen 不等式的等号成立，因此：

$$
l(\theta^{(t)})=\sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}
$$

参数 $\theta^{(t+1)}$  可以通过对上面等式中等号右侧进行最大化而得到（注意(4)和(5)中$\theta的上标$）。因此： 

$$
\begin{aligned}
l(\theta^{(t+1)}) &\ge \sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t+1)})}{Q_i^{(t)}(z^{(i)})} &(4)\\
&\ge \sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})} &(5)\\
&= l(\theta^{(t)}) &(6)
\end{aligned}
$$

上面的第一个不等式推自：

$$
l(\theta)\ge \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

上面这个不等式对于任意值的 $Q_i$ 和 $\theta$ 都成立，尤其当 $Q_i = Q_i^{(t)}, \theta = \theta^{(t+1)}$。要得到等式 $(5)$，我们要利用 $\theta^{(t+1)}$ 的选择能够保证：

$$
arg\max_\theta \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

这个式子对 $\theta^{(t+1)}$ 得到的值必须大于等于 $\theta^{(t)}$ 得到的值提供了保证。最后，推导等式$(6)$ 的这一步，正如之前所示，因为在选择的时候我们选的 $Q_i^{(t)}$ 就是要保证 Jensen 不等式对 $\theta^{(t)}$ 等号成立。

因此，$EM$ 算法就能够导致似然函数的单调收敛。在我们推导 $EM$ 算法的过程中，我们要一直运行该算法到收敛。得到了上面的结果之后，可以使用一个合理的收敛检测来检查在成功的迭代之间的 $l(\theta)$ 的增长是否小于某些容忍参数，如果 $EM$ 算法对 $l(\theta)$ 的增大速度很慢，就声明收敛。

**备注。** 如果我们定义

$$
J(Q, \theta)=\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

通过我们之前的推导，就能知道 $l(\theta) ≥ J(Q, \theta)$。这样 $EM$ 算法也可看作是在 $J$ 上的坐标上升，其中 $E$ 步骤在 $Q$ 上对 $J$ 进行了最大化（自己检查哈），然后 $M$ 步骤则在 $\theta$ 上对 $J$ 进行最大化。

### 高斯混合模型回顾

有了对 $EM$ 算法的广义定义之后，我们就可以回顾一下之前的高斯混合模型问题，其中要拟合的参数有 $\phi, \mu$ 和$\Sigma$。为了避免啰嗦，这里就只给出在 $M$ 步骤中对$\phi$ 和 $\mu_j$ 进行更新的推导。

$E$ 步骤很简单。还是按照上面的算法推导过程，只需要计算：

$$
w_j^{(i)}=Q_i(z^{(i)}=j)=P(z^{(i)}=j \mid x^{(i)};\phi,\mu,\Sigma)
$$

这里面的 $“Q_i(z^{(i)} = j)”$ 表示的是在分布 $Q_i$上 $z^{(i)}$ 取值 $j$ 的概率。

接下来在 $M$ 步骤，就要最大化关于参数 $\phi,\mu,\Sigma$的值： 

$$
\begin{aligned}
\sum_{i=1}^m&\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\phi,\mu,\Sigma)}{Q_i(z^{(i)})}\\
&= \sum_{i=1}^m\sum_{j=1}^kQ_i(z^{(i)}=j)log\frac{p(x^{(i)} \mid z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{Q_i(z^{(i)}=j)} \\
&= \sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}log\frac{\frac{1}{(2\pi)^{n/2}\mid \Sigma_j \mid^{1/2}}exp(-\frac 12(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\cdot\phi_j}{w_j^{(i)}}
\end{aligned}
$$

先关于 $\mu_l$ 来进行最大化。如果去关于 $\mu_l$ 的（偏）导数（derivative），得到：

$$
\begin{aligned}
\nabla_{\mu_l}&\sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}log\frac{\frac{1}{(2\pi)^{n/2}\mid\Sigma_j\mid^{1/2}}exp(-\frac 12(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j))\cdot\phi_j}{w_j^{(i)}} \\
&= -\nabla_{\mu_l}\sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}\frac 12(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j) \\
&= \frac 12\sum_{i=1}^m w_l^{(i)}\nabla_{\mu_l}2\mu_l^T\Sigma_l^{-1}x^{(i)}-\mu_l^T\Sigma_l^{-1}\mu_l  \\
&= \sum_{i=1}^m w_l^{(i)}(\Sigma_l^{-1}x^{(i)}-\Sigma_l^{-1}\mu_l)
\end{aligned}
$$

设上式为零，然后解出 $\mu_l$ 就产生了更新规则（update rule）：

$$
\mu_l := \frac{\sum_{i=1}^m w_l^{(i)}x^{(i)}}{\sum_{i=1}^m w_l^{(i)}}
$$


再举一个例子，推导在 $M$ 步骤中参数 $\phi_j$ 的更新规则。把仅关于参数 $\phi_j$ 的表达式结合起来，就能发现只需要最大化下面的表达式：

$$
\sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}log\phi_j
$$

然而，还有一个附加的约束，即 $\phi_j$ 的和为$1$，因为其表示的是概率 $\phi_j = p(z^{(i)} = j;\phi)$。为了保证这个约束条件成立，即 $\sum^k_{j=1}\phi_j = 1$，我们构建一个拉格朗日函数（Lagrangian）：

$$
\mathcal L(\phi)=\sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}log\phi_j+\beta(\sum^k_{j=1}\phi_j - 1)
$$

其中的 $\beta$ 是 拉格朗日乘数（Lagrange multiplier）$^2$ 。求导，然后得到： 

$$
\frac{\partial}{\partial{\phi_j}}\mathcal L(\phi)=\sum_{i=1}^m\frac{w_j^{(i)}}{\phi_j}+1
$$

>2 这里我们不用在意约束条件 $\phi_j \ge 0$，因为很快就能发现，这里推导得到的解会自然满足这个条件的。

设导数为零，然后解方程，就得到了：

$$
\phi_j=\frac{\sum_{i=1}^m w_j^{(i)}}{-\beta}
$$

也就是说，$\phi_j \propto \sum_{i=1}^m w_j^{(i)}$。结合约束条件$\Sigma_j \phi_j = 1$，可以很容易地发现 $-\beta = \sum_{i=1}^m\sum_{j=1}^kw_j^{(i)} = \sum_{i=1}^m 1 =m$. （这里用到了条件 $w_j^{(i)} =Q_i(z^{(i)} = j)$，而且因为所有概率之和等于$1$，即$\sum_j w_j^{(i)}=1$）。这样我们就得到了在 $M$ 步骤中对参数 $\phi_j$ 进行更新的规则了：

$$
\phi_j := \frac 1m \sum_{i=1}^m w_j^{(i)}
$$

接下来对 $M$ 步骤中对 $\Sigma_j$ 的更新规则的推导就很容易了。 
