---
layout:     post
title:      CS229-学习理论
subtitle:   机器学习
date:       2019-12-15
author:     Pkun
header-img: img/stfml.jpg
catalog: true
tags:
    - 机器学习
---



大部分内容来自CS229吴恩达老师的课程讲义，感谢翻译

| 原作者 | 翻译 | 校对 |
| --- | --- | --- |
| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |


|相关链接|
|---|
|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|
|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|
|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|
|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|


# 学习理论

## 预先准备

### 泛化误差

一个推测模型（hypothesis）的**泛化误差（generalization error）**（稍后再给出正式定义），正是那些不属于训练集的样本潜在的预期偏差（expected error on examples not necessarily in the training set）。

### 偏差（欠拟合）

**偏差（bias）** 定义为预期的泛化误差（expected generalization error），即便我们要去拟合的对象是一个非常大的甚至是无限的训练数据集。

### 方差（过拟合）

很可能我们基于数据拟合出来的模型可能碰巧只适合于眼下这个小规模的有限的训练集，而并不能反映 $x$ 和 $y$ 之间更广泛的关系。通过对训练集拟合得到的这个“不太靠谱的（spurious）”的模式，我们得到的可能也就是一个有很大泛化误差（large generalization error）的模型。这样的话，我们就说这个模型的**方差**很大（large variance）$^1$。



通常情况下，咱们需要在偏差（bias）和方差（variance）之间进行权衡妥协。如果我们的模型过于“简单（simple）”，而且参数非常少，那这样就可能会有很大的偏差（bias），而方差（variance）可能就很小；如果我们的模型过于“复杂（complex）”，有非常多的参数，那就可能反过来又特别大的方差（variance），而偏差（bias）就会小一些。在上面三种不同拟合的样例中，用二次函数来进行拟合得到的效果，明显是胜过一次线性拟合，也强于五次多项式拟合。


### 引理

引理1 (联合约束，The union bound)。设 $A_1, A_2, ..., A_k$ 是 $k$个不同事件（但不一定互相独立），则有：

$$
P(A_1\cup...\cup A_k)\leq P(A_1)+...+P(A_k)
$$

引理2 (Hoeffding 不等式) 。设 $Z_1,...,Z_m$ 是 $m$ 个独立的并且共同遵循伯努利分布（Bernoulli($\phi$) distribution）的随机变量（independent and identically distributed (iid) random variables）。例如：$P(Z_i =1)=\phi$ 而 $P(Z_i =0)= 1 - \phi$. 设 $\hat\phi=(\frac1m)\sum^m_{i=1}Z_i$ 是这些随机变量的平均值，然后设任意的 $\gamma \geq 0$ 为某一固定值（fixed），则有：

$$
P(|\phi-\hat\phi|>\gamma)\leq 2\exp (-2\gamma^2m)
$$

上面这个引理（在机器学习理论里面也称为 **切尔诺夫约束，Chernoff bound** ）表明，如果我们我们从一个伯努利分布的随机变量中选取平均值 $\hat\phi$ 来作为对 $\phi$ 的估计值，那么只要 $m$ 足够大，我们偏移真实值很远的概率就比较小。另外一种表述方式是：如果你有一个有偏差的硬币（biased coin），抛起来落下人头朝上的概率是 $\phi$，如果你抛了 $m$ 次，然后计算人头朝上的比例，若 $m$ 非常大，那么这个比例的值，就是一个对 $\phi$ 的一个概率很高的很好的估计。



### 经验误差（empirical error）

假设我们有一个给定的训练集 $S = \{(x^{(i)},y^{(i)});i = 1,...,m\}$，其样本规模为 $m$，集合中的训练样本 $(x^{(i)},y^{(i)})$ 是服从某概率分布 $D$ 的独立且同分布的随机变量。设一个假设（hypothesis）为$h$，我们则用如下的方法定义训练误差（也称为学习理论中的**经验风险 empirical risk** 或者**经验误差 empirical error）**：

$$
\hat\epsilon(h) =\frac1m\sum^m_{i=1}1\{h(x^{(i)})\neq y^{(i)}\}
$$

这个值只是假设模型 $h$ 分类错误样本占据训练样本总数的分数。

### 泛化误差（generalization error）

可以定义泛化误差（generalization error）为：

$$
\epsilon(h) =P_{(x,y)\sim D}(h(x)\neq y)
$$

$\epsilon(h)$ 的这个定义实际上也就相当于，基于分布 $D$ 给出的一个新的样本 $(x, y)$ ，假设模型 $h$ 对该样本分类错误的概率。


要注意，这里我们有一个预先假设，也就是训练集的数据与要用来检验假设用的数据都服从同一个分布 $D$（这一假设存在于对泛化误差的定义中）。这个假设通常也被认为是 PAC 假设之一。

> PAC 是一个缩写，原型为“probably approximately correct”，这是一个框架和一系列假设的集合，在机器学习理论中的很多结构都是基于这些假设而证明得到的。这个系列假设中**最重要的两个，就是训练集与测试集服从同一分布，以及训练样本的独立性。**


### 经验风险最小化（ERM）

假设 $h_\theta (x) = 1\{\theta^T x \geq 0\}$。拟合参数 $\theta$ 的一个思路就是可以使训练误差（training error）最小化，然后选择取最小值的时候的 $\theta$ ：

$$
\hat\theta=arg\min_\theta\hat\epsilon(h_\theta)
$$

我们把上面这个过程称之为**经验风险最小化**（empirical risk minimization，缩写为 ERM），而这种情况下通过学习算法得到的假设结果就是 $\hat h = h_{\hat\theta}$ 。


我们把通过学习算法所使用的**假设类（hypothesis class）$H$** 定义为所有分类器的集合（set of all classifiers）。对于线性分类问题来说，$H = \{h_\theta : h_\theta(x) = 1\{\theta^T x \geq 0\}, \theta \in R^{n+1}\}$，是一个对 $X$（输入特征） 进行分类的所有分类器的集合，其中所有分类边界为线性。更广泛来说，假设我们研究神经网络（neural networks），那么可以设 $H$ 为能表示某些神经网络结构的所有分类器的集合。

现在就可以把 经验风险最小化（ERM）看作是对函数类 $H$ 的最小化，其中由学习算法来选择假设（hypothesis）：

$$
\hat h=arg\min_{h\in H}\hat\epsilon(h)
$$

## 有限个假设时

假设类 $H = \{h_1, ..., h_k\}$，由 $k$ 个不同假设组成。因此，$H$ 实际上就是由 $k$ 个从输入特征 $X$ 映射到 $\{0, 1\}$ 的函数组成的集合，而经验风险最小化（ERM）就是从这样的 $k$ 个函数中选择训练误差最小（smallest training error）的作为 $\hat h$。

我们希望能够确保 $\hat{h}$ 的泛化误差。这需要两个步骤：首先要表明 $\hat\epsilon(h)$ 是对所有 $h$ 的 $\epsilon(h)$ 的一个可靠估计。其次就需要表明这个 $\hat\epsilon(h)$ 位于 $\hat{h}$ 泛化误差的上界。


任选一个固定的 $h_i \in H$。假如有一个伯努利随机变量（Bernoulli random variable） $Z$，其分布入下面式中定义。 然后我们从 $D$ 中取样 $(x, y)$，并设 $Z=1\{h_i(x)\neq y\}$。类似地，我们还定义了一个 $Z_j=1\{h_i(x^{(j)})\neq y^{(j)}\}$。由于我们的训练样本都是从 $D$ 中取来的独立随机变量（iid），所以在此基础上构建的 $Z$ 和 $Z_j$ 也都服从相同的分布。

然后，就可以把训练误差写成下面这种形式：

$$
\hat\epsilon(h_i)=\frac 1m\sum_{j=1}^mZ_j
$$

因此，$\hat\epsilon(h_i)$ 就正好是 $m$ 个随机变量 $Z_j$ 的平均值，而 $Z$ (以及 $Z_j$) 的期望值（expected value）正好就是$\epsilon(h)$，接下来，就可以使用 Hoeffding 不等式，得到下面的式子：

$$
P(|\epsilon(h_i)-\hat\epsilon(h_i)|>\gamma)\leq 2\exp (-2\gamma^2m)
$$

假如训练样本的规模 $m$ 规模很大的时候，训练误差有很接近泛化误差（generalization error）的概率是很高的。然而我们不仅仅满足于针对某一个特定的 $h_i$ 的时候能保证 $\epsilon(h_i)$ 接近 $\hat\epsilon(h_i)$ 且接近的概率很高。我们还要证明同时针对所有的 $h \in H$ 这个结论都成立。 为了证明这个结论，我们设 $A_i$ 来表示事件 $|\epsilon(h_i) - \hat\epsilon(h_i)| > \gamma$。我们已经证明了，对于任意给定的 $A_i$，都有 $P(A_i) \le 2\exp (-2\gamma^2m)$ 成立。接下来，使用联合约束（union bound），就可以得出下面的关系：

$$
\begin{aligned}
P(\exists h\in H.|\epsilon(h_i)-\hat\epsilon(h_i)|>\gamma)& = P(A_1\cup...\cup A_k) \\
                                                   & \le \sum_{i=1}  ^k P(A_i) \\
                                                   & \le \sum_{i=1}^k 2\exp (-2\gamma^2m) \\
                                                   & = 2k\exp (-2\gamma^2m)
\end{aligned}
$$

如果等式两边都用 1 来减去原始值（subtract both sides from 1），则不等关系改变为：

$$
\begin{aligned}
P(\neg\exists h\in H.|\epsilon(h_i)-\hat\epsilon(h_i)|>\gamma)& 
= P(\forall h\in H.|\epsilon(h_i)-\hat\epsilon(h_i)|\le\gamma) \\
& \ge 1-2k\exp (-2\gamma^2m)
\end{aligned}
$$

至少有 $1-2k exp(-2\gamma^2 m)$ 的概率，我们能确保对于所有的 $h \in H$，$\epsilon(h)$ 在 $\hat\epsilon(h)$ 附近的 $\gamma$ 范围内。这种结果就叫做一致收敛结果（uniform convergence result），因为这是一个针对所有的 $h \in H$ 都同时成立的约束（与之相反的是只针对某一个 $h$ 才成立的情况）。

这里我们感兴趣的变量（quantities of interest）有三个：$m$, $\gamma$, 以及误差的概率（probability of error）；我们可以将其中的任意一个用另外两个来进行约束（bound either one in terms of the other two）。

给定一个 $\gamma$ 以及某个 $\delta \geq 0$，如果要保证训练误差处于泛化误差附近 $\gamma$ 的范围内的概率最小为 $1 – \delta$，那么 $m$ 应该要多大呢？可以设 $\delta = 2k exp(-2\gamma^2 m)$ 然后解出来 $m$，然后我们就发现如果有：

$$
m\ge \frac{1}{2\gamma^2}log\frac{2k}{\delta}
$$

并且概率最小为 $1-\delta$，就能保证对于所有的 $h \in H$ 都有 $|\epsilon(h) - \hat\epsilon(h)| ≤ \gamma$ 。这种联合约束也说明了需要多少数量的训练样本才能对结果有所保证。

是某些特定的方法或者算法所需要训练集的规模 $m$ 来实现一定程度的性能，这样的训练集规模 $m$ 也叫做此类算法的**样本复杂度** 。

上面这个约束的关键特性在于要保证结果，所需的训练样本数量只有 $k$ 的对数，$k$ 即假设集合 $H$ 中的假设个数。这以特性稍后会很重要。

同理，我们也可以将 $m$ 和 $\delta$ 设置为固定值，然后通过上面的等式对 $\gamma$ 进行求解，然后表明对于所有的 $h \in H$ ，都有概率为 $1 –\delta$（这里还是要你自己去证明了，不过你相信这个是对的就好了。）。

$$
|\hat\epsilon(h)-\epsilon(h)|\le \sqrt{\frac{1}{2m}log\frac{2k}{\delta}}
$$

现在，我们假设这个联合收敛成立（uniform convergence holds），也就是说，对于所有的 $h \in H$，都有 $|ε(h)-\hat\epsilon(h)| ≤ \gamma$。我们的学习算法选择了 $\hat{h} = arg\min_{h\in H} \hat\epsilon(h)$，关于这种算法的泛化，我们能给出什么相关的证明呢？

将 $h^∗ = arg \min_{h\in H} \epsilon(h)$ 定义为 $H$ 中最佳可能假设（best possible hypothesis）。这里要注意此处的 $h^∗$ 是我们使用假设集合 $H$ 所能找出的最佳假设，所以很自然地，我们就能理解可以用这个 $h^∗$ 来进行性能对比了。则有：

$$
\begin{aligned}
\epsilon(\hat h) & \le \hat\epsilon(\hat h)+\gamma \\
                 & \le \hat\epsilon(h^*)+\gamma \\
                 & \le \epsilon(h^*)+2\gamma 
\end{aligned}
$$

上面的第一行用到了定理 $| \epsilon(\hat h) - \hat\epsilon (\hat h) | \le \gamma$（可以通过上面的联合收敛假设来推出）。第二行用到的定理是 $\hat{h}$ 是选来用于得到最小 $\hat\epsilon(h)$ ，然后因此对于所有的 $h$ 都有 $\hat\epsilon(\hat{h}) \leq \hat\epsilon(h)$，也就自然能推出 $\hat\epsilon(\hat{h}) \le \hat\epsilon(h^∗)$ 。第三行再次用到了上面的联合收敛假设，此假设表明 $\hat\epsilon(h^∗) \le \epsilon(h^∗) + \gamma$ 。所以，我们就能得出下面这样的结论：如果联合收敛成立，那么 $\hat h$ 的泛化误差最多也就与 $H$ 中的最佳可能假设相差 $2\gamma$。


**定理:** 设 $|H| = k$，然后设 $m$ 和 $\delta$ 为任意的固定值。然后概率至少为 $1 - \delta$，则有：

$$
\epsilon(\hat h)\le \min_{h\in H}\epsilon(h)+2\sqrt{\frac{1}{2m}log\frac{2k}{\delta}}
$$

上面这个的证明，可以通过令 $\gamma$ 等于平方根$\sqrt{\cdot}$的形式，然后利用我们之前得到的概率至少为 $1 – \delta$ 的情况下联合收敛成立，接下来利用联合收敛能表明 $\epsilon(h)$ 最多比 $\epsilon(h^∗) = \min_{h\in H} \epsilon(h)$ 多 $2\gamma$。

这也对我们之前提到过的在模型选择的过程中在偏差（bias）/方差（variance）之间的权衡给出了定量方式。例如，加入我们有某个假设类 $H$，然后考虑切换成某个更大规模的假设类 $H' \supseteq H$。如果我们切换到了 $H'$ ，那么第一次的 $\min_h \epsilon(h)$ 只可能降低（因为我们这次在一个更大规模的函数集合里面来选取最小值了）。因此，使用一个更大规模的假设类来进行学习，我们的学习算法的“偏差（bias）”只会降低。然而，如果 $k$ 值增大了，那么第二项的那个二倍平方根项$2\sqrt{\cdot}$也会增大。这一项的增大就会导致我们使用一个更大规模的假设的时候，“方差（variance）”就会增大。

通过保持 $\gamma$ 和 $\delta$ 为固定值，然后像上面一样求解 $m$，我们还能够得到下面的样本复杂度约束：

**推论（Corollary）：** 设 $|H| = k$ ，然后令 $\delta,\gamma$ 为任意的固定值。对于满足概率最少为 $1 - \delta$ 的 $\epsilon(\hat{h}) \le min_{h\in H} \epsilon(h) + 2\gamma$ ，下面等式关系成立：

$$
\begin{aligned}
m &\ge \frac{1}{2\gamma^2}log\frac{2k}{\delta} \\
  & = O(\frac{1}{\gamma^2}log\frac{k}{\delta})    
\end{aligned}
$$

## 无限个假设时

很多假设类包含了无限个假设，这时候继续采用上面的结论就不对了，但是对于无限个假设类的情况，也有一些类似的结论。

若我们有一个假设集合 $H$，使用 $d$ 个实数来进行参数化。由于我们使用计算机表述实数，这需要64个bit。这样我们的这个假设类实际上包含的不同假设的个数最多为 $k = 2^{64d}$ 。结合上一节的最后一段那个推论（Corollary），我们就能发现，要保证 $\epsilon(\hat{h}) \leq \epsilon(h^∗) + 2\gamma$ ，同时还要保证概率至少为 $1 - \delta$ ，则需要训练样本规模 $m$ 满足$m \ge O(\frac{1}{\gamma^2}log\frac{2^{64d}}{\delta})=O(\frac{d}{\gamma^2}log\frac{1}{\delta})=O_{\gamma,\delta}(d)$（这里的 $\gamma$，$\delta$下标表示最后一个大$O$可能是一个依赖于$\gamma$和$\delta$的隐藏常数。）因此，所需的训练样本规模在模型参数中最多也就是线性的。

如果我们试图使训练误差（training error）最小化，那么为了使用具有 dd 个参数的假设类（hypothesis class）的学习效果“较好（well）”，通常就需要按照 dd 的线性数量来确定训练样本规模。

接下来让我们去掉对$H$的参数化

给定一个点的集合 $S = \{x^{(i)}, ..., x^{(d)}\}$（与训练样本集合无关），其中 $x(i) \in X$，如果 $H$ 能够对 集合 $S$ 实现任意的标签化（can realize any labeling on S），则称 **$H$ 打散（shatter）** 了 $S$。例如，对于任意的标签集合 （set of labels）$\{y^{(1)}, ..., y^{(d)}\}$，都有 一些$h\in H$ ，对于所有的$i = 1, ...d$，式子$h(x^{(i)}) = y^{(i)}$都成立。（译者注：关于 shattered set 的定义可以参考：[*https://en.wikipedia.org/wiki/Shattered\_set*](https://en.wikipedia.org/wiki/Shattered_set) 更多关于 VC 维 的内容也可以参考：[*https://www.zhihu.com/question/38607822*](https://www.zhihu.com/question/38607822) ）

给定一个假设类 $H$，我们定义其 **VC维度（Vapnik-Chervonenkis dimension），** 写作 $VC(H)$，这个值也就是能被 $H$ 打散（shatter）的最大的集合规模。（如果 $H$ 能打散任意大的集合（arbitrarily large sets），那么 $VC(H) = ∞$。）

具体来看则如下图所示，以下八种分类情况中的任意一个，我们都能找到一种用能够实现 “零训练误差（zero training error）” 的线性分类器（linear classifier）：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note4f3.png)

此外，这也有可能表明，这个假设类 $H$ 不能打散（shatter）4 个点构成的集合。因此，$H$ 可以打散（shatter）的最大集合规模为 3，也就是说 $VC（H）= 3$。

这里要注意，HH 的 VCVC 维 为3，即便有某些 3 个点的集合不能被 HH 打散。例如如果三个点都在一条直线上，那就没办法能够用线性分类器来对这三个点的类别进行划分了（如下图右侧所示）。

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note4f4.png)

这样就能够给出下面的定理（theorem）了，该定理来自 Vapnik。（有不少人认为这是所有学习理论中最重要的一个定理。）

Theorem. Let H be given, and let d = VC(H). Then with probability at least 1-δ, we have that for all h∈H,

定理：给定 $H$，设 $d = VC(H)$。然后对于所有的 $h\in H$，都有至少为 $1-\delta$ 的概率使下面的关系成立：

$$
|\epsilon(h)-\hat\epsilon(h)|\le O(\sqrt{\frac{d}{m}log\frac{d}{m}+\frac 1mlog\frac 1\delta})
$$

此外，有至少为 $1-\delta$ 的概率：

$$
\hat\epsilon(h)\le \epsilon(h^*)+O(\sqrt{\frac{d}{m}log\frac{d}{m}+\frac 1mlog\frac 1\delta})
$$

换句话说，如果一个假设类有有限的 $VC$ 维，那么只要训练样本规模 $m$ 增大，就能够保证联合收敛成立（uniform convergence occurs）。和之前一样，这就能够让我们以 $\epsilon(h)$ 的形式来给 $\epsilon(h^∗)$ 建立一个约束（bound）。此外还有下面的推论（corollary）：


**推论（Corollary）：** 对于所有的 $h \in H$ 成立的 $|\epsilon(h) - \hat\epsilon(h)| \le \gamma$ （因此也有 $\epsilon(\hat h) ≤ \epsilon(h^∗) + 2\gamma$），则有至少为 $1 – \delta$ 的概率，满足 $m = O_{\gamma,\delta}(d)$。


换个方式来说，要保证使用 假设集合 $H$ 的 机器学习的算法的学习效果“良好（well）”，那么训练集样本规模 $m$ 需要与 $H$ 的 $VC$ 维度 线性相关（linear in the VC dimension of H）。这也表明，对于“绝大多数（most）”假设类来说，（假设是“合理（reasonable）”参数化的）$VC$ 维度也大概会和参数的个数线性相关。把这些综合到一起，我们就能得出这样的一个结论：对于一个试图将训练误差最小化的学习算法来说：训练样本个数 通常都大概与假设类 $H$ 的参数个数 线性相关。

### PS

虽然我懵可以将训练误差最小化的训练样本个数与$H$的参数个数关联起来，但是实际上这是一个非常非常弱的上界（固定一个概率，参数个数，然后带入公式中计算会得到一个超大的数），原因在于我们证明的过程中假设了样本服从任意一种分布，这是一个非常弱的条件，实际上在真实的环境中，我们的样本往往都服从某种分布，这会让这个上界下降不少。所以其实我们并不需要过多的样例，


