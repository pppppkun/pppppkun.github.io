---
layout:     post
title:      矩阵求导术
subtitle:   数学
date:       2019-11-7
author:     Pkun
header-img: img/matrix-head.jpg
catalog: true
tags:
    - 线性代数
    - 机器学习
---

# 前言

矩阵求导的技术，在统计学、控制论、机器学习等领域有广泛的应用。在本人阅读西瓜书的过程中发现这项技术非常的重要。在学习了一段时间后，准备写一篇博客来总结一下。分作两篇，上篇讲标量对矩阵的求导术，下篇讲矩阵对矩阵的求导术。本文使用小写字母$x$表示标量，粗体小写字母 $\pmb x$ 表示（列）向量，大写字母$X$表示矩阵。


对于标量$f$对矩阵$X$的导数，我们定义其为 $\frac{\partial f}{\partial X} = [\frac{\partial f}{\partial X_{ij}}]$

即$f$对$X$逐个元素求导并排成和$X$一样尺寸的矩阵，然而这个定义在计算中并不好用，原因是因为对函数比较复杂时，对逐个元素求导会很麻烦（可以想象对一堆元素求偏微分）


为什么这么麻烦呢？很容易想到矩阵是一个整体，我们对逐个元素求导显然破坏了它的整体性，那如果我们从整体出发呢？

在一元微积分中我们有导数与微分的关系 $d f =f'(x)dx$，在多元微积分中，我们有也有导数和微分的关系 $df = \sum_{i=1}^n \frac{\partial f}{\partial x_i} dx_i$，即全微分

进一步的，我们还有 $ \sum_{i=1}^n \frac{\partial f}{\partial x_i} dx_i = (\frac{\partial f}{\partial \pmb{x}})^T d\pmb{x}$

在这里我们将$df$看成一个n*1的向量，因此，全微分可以看成$ \frac{\partial f}{\partial \pmb{x}}$ 和 $d \pmb{x}$做内积。

根据以上的结论，我们将矩阵导数和微分建立联系，有 $df = \sum _{i=1}^m \sum_{j=1}^n \frac{\partial f}{\partial X_{ij}} d{X_{ij}}$

进一步的，我们可以推导出上式即为$tr((\frac{\partial f}{\partial X})^T dX)$（相同尺寸的矩阵的内积为某一矩阵转置与另一矩阵的积的迹)


现在我们来建议运算法则

- $d(X +Y) = dX + dY$ $d(X-Y)=dX-dY$
- $d(XY) = dXY + XdY$
- $d(X^T) = (dX)^T$
- $dX^{-1} = (dX)^{-1}$
- $d|X| = tr(X^*dX)$ 这个可以用Laplace展开证明
    > 张贤达.《矩阵分析与应用》.p279
- $d(X\odot Y) = dX \odot Y + X \odot dY$ 符号$\odot$表示逐元素相乘

- $d \sigma (X) = \sigma'(X) \odot dX, \sigma(X) = [\sigma(X_{ij})]$是逐元素标量函数运算
$$ X =
 \left[
 \begin{matrix}
   X_{11} & X_{12}\\
   X_{21} & X_{22}\\
  \end{matrix}
  \right]
$$ 
$$
 dsin(X) = 
 \left[
    \begin{matrix}
    cosX_{11}dX_{11} & cosX_{12}dX_{12} \\
    cosX_{21}dX_{21} & cosX_{22}dX_{22} \\
    \end{matrix}
    \right]
= cos(X) \odot dX
$$

(未完待续)



