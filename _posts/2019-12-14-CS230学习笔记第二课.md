---
layout:     post
title:      CS230-Practical Approaches to Deep Learning Projects
subtitle:   Deep learning
date:       2019-12-14
author:     Pkun
header-img: img/stfml.jpg
catalog: true
tags:
    - Deep Learning
---

# 写在前面

本文内容根据CS230-2018fall的课堂视频以及讲义做了一些总结
[课程网址](https://cs230.stanford.edu/)

## learning process

在构建深度学习系统之前，我们先考虑一个深度学习系统具有哪些特性
- Input
    - img,text,vedio or something like that.
- Model
    - Model = Architecture + Parameters
    - Architecture 是模型中神经网络的结构，like CNN or RNN
    - Parameters 可以当成是不同层之间的权重等
- Output
    - 可以是一些图片（后面会看到栗子）
    - 可以是 1/0 来判断是否属于某一类
    - ...
- Loss
    - Gradients
    - Back-Propagation

以上在建立模型的时候基本都是确定好的（Parameters会不断训练），在模型中还有些是不确定的，比如
- Activation function
    - ReLU
    - Sigmoid
- Optimizer
- Hyper-Parameters

## encoding

encoding 意思大概就是将一些lower level representation of data 通过 神经网络 组合成一些能表示更详细的信息的high level representation of data.

比如从 像素级的数据->边缘->眼睛、鼻子、嘴巴->脸 就是一种encoding

> 其实我也不是很清楚encoding到底是指的什么，感觉他讲的有点含糊。

在深度学习中，特征学习取代了特征工程

## Today's outline
- Analyze a problem from a deep learning approach
- Choose an architecture
- Choose a loss and training strategy

课堂上讲了四个栗子，下面会一一介绍，同时会通过对不同问题的来获取一些经验，通常考虑以下几个问题
- Data?
- Input?
- Output?
- Architecture?
- Loss?

### Day'n'Night classification

***Goal*** : 通过一张图片，判断是白天还是黑夜

- Data
    - 数据显然是一些在白天或者黑天拍摄的图片，当然对于数据集的大小你要根据当前你的问题的复杂度来估计，在这个问题中，如果你打算用一个比较简单的神经网络，大概需要10000张照片。
- Input
    - 输出的是一张新的图片
    - 在这里我们需要注意一些问题，比如图片的分辨率，在确定分辨率这件事情上，我们可以先事先确定几个分辨率，然后把图片交给人来判断，看哪个效果是最好的，就把图片设置成那个分辨率。
- Output
    - 0/1
    - 最后的Activation Function应该是sigmoid
- Architecture
    - 比较浅或者简单的神经网络就可以工作的很好了
- Loss
    - $ L = -[y\log(\hat y) + (1-y)\log(1-\hat y)]$

### Summary of learnings: Day n’ Night classification

> - Use a known proxy project to evaluate how much data you need.
> - Be scrappy. For example, if you’d like to find a good resolution of images to use
for your data, but don’t have time for a large scale experiment, approximate
human-level performance by testing your friends as classifiers.


## Face Verification and Recognition

***Goal*** ：  A school wants to use Face Verification for validating student IDs in facilities 

- Data
    - 应该是每个学生的照片，并且标上他们的名字
- Input
    - 摄像头拍到的一张照片
    - 分辨率通常是412*412
- Output
    - 1/0
- Architecture
    - 直觉上来看我们应该对比每个像素的差
    - issues:
        - 背景颜色？
        - 人的肤色可能会变化
        - ID photo更新不及时
    - 基于问题我们应该encode information about a picture in a vector，然后衡量两个向量之间的距离 

> We gather all student faces encoding in a database. Given a new
picture, we compute its distance with the encoding of card holder

- Loss?Traning?
    - 我们希望同一个人的两张照片的vector之间的距离尽可能小，不同人的两张照片之间的距离尽可能大
    - $ L = \mid\mid Enc(A) - Enc(p) \mid\mid^2_2 - \mid\mid Enc(A)-Enc(N) \mid\mid^2_2 + \alpha $
> Enc(A)是数据库中照片的向量，Enc(p)是新照片的，Enc(N)是别人的，$\alpha$是一个避免损失函数在等于0而无法下降（没有理解）的常数，通常我们希望损失函数大于等于0

### Recognition

上面那个是人脸认证，这个是人脸识别，两者之间还是有一些细微的差别

在人脸识别的时候我们在数据库中多存几张同一学生的照片，然后通过`K-Nearest Neighbors`来识别学生。

在识别你的亲人的时候，手机里面可能有一个`K-Means`或者`X-Means`算法来将照片划分到同一类中

### Summary of learnings: Face Recognition

> - In face verification, we have used an encoder network to learn a lower
dimensional representation (called “encoding”) for a set of data by training the
network to focus on non-noisy signals.
> - Triplet loss is a loss function where an (anchor) input is compared to a
positive input and a negative input. The distance from the anchor input to the
positive input is minimized, whereas the distance from the anchor input to the
negative input is maximized.
> - You learnt the difference between face verification, face identification and
face clustering.

## Art generation (Neural Style Transfer)

***Goal*** ： Given a picture, make it look beautiful

- Data 
    - 所有的图片
- Input
    - 一张图片的内容
    - 一张图片的风格
- Output
    - 生成的一张图片
- Architecture
    - 在这个任务中我们不训练一个模型来提取图片中的信息，而是使用一个预训练好的模型来提取
    - 可能使用图片会更加清楚

![](https://raw.githubusercontent.com/pppppkun/pppppkun.github.io/master/img/CS230-1.png)

![](https://raw.githubusercontent.com/pppppkun/pppppkun.github.io/master/img/CS230-2.png)



> Gram Matrix是一种提取图像风格的工具

- Loss
    - $ L = \mid\mid Content_c - Content_G \mid\mid^2_2 + \mid\mid Style_S - Style_G \mid\mid^2_2$

### Summary of learnings: Neural Style Transfer

> - In the neural style transfer algorithm proposed by Gatys et al., you optimize
image pixels rather than model parameters. Model parameters are
pretrained and non-trainable.
> - You leverage the “knowledge” of a pretrained model to extract the content of a
content image and the style of a style image.
> - The loss proposed by Gatys et al. aims to minimize the distances between the
content of the generated and content images, and the style of the generated
and style images.

## Trigger word detection

***Goal*** ： Given a 10sec audio speech, detect the word "activate".

- Data
    - 10秒的音频
- Input
    - 同上，速率应该相似
- Output
    - 我们有两种标记方法，一种是0/1，另一种是y = 0000...1..000
    - 后一种方式对人更友好，他标明了这个单词出现的地方
- Architecture
    - Sounds like it should be RNN
- Loss
    - $ L = -[y\log(\hat y) + (1-y)\log(1-\hat y)] $

### Summary of learnings: Trigger word detection

> - Your data collection strategy is critical to the success of your project. (If applicable) Don’t hesitate to get out of the building.
> - You can gain insights on your labelling strategy by using a human experiment.
> - Refer to expert advice to earn time and be guided towards a good direction.


